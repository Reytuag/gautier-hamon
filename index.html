<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Gautier Hamon</title>

    <meta name="author" content="Gautier Hamon">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Gautier Hamon
                </p>
                <p>I completed my PhD at Inria <a href="https://flowers.inria.fr/">FLOWERS team</a> (France) working on open-ended skill acquisition and major transitions in simulated environments: <a href="data/thesis_hamon_gautier.pdf"> Thesis pdf </a>.
                  Including work on reinforcement learning and meta reinforcement learning, evolutionary simulations and self-organization in cellular automata. At the beginning of 2024 I did a 3 month visit in <a href="http://complex.upf.edu/ricard-sol%C3%A9">Ricard Solé's complex system lab</a>.
                </p>
                <p> I have a machine learning background. I obtained my MSc <a href="https://www.master-mva.com/">MVA (Mathematics, Vision, Learning)</a> from Institut Polytechnique de Paris/ENS Paris-Saclay, with highest honors. As well as my Master of Engineering (MEng) from <a href="https://www.telecom-paris.fr/en/home">Télécom Paris</a> (GPA 4.). </p>
                <p>
                
                </p>
                <p style="text-align:center">
                  <a href="mailto:gautier.hamon@inria.fr">Email</a> &nbsp;/&nbsp;
                  <a href="data/gautierhamonCV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=VvWzeMwAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/hamongautier">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Reytuag/">Github</a>
                </p>
              </td>

              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/gautierhamon.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/gautierhamon.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <h3> Research interests</h3>

                <li> <b>Reinforcement learning </b>: Meta-RL, continual learning, open-ended skill acquisition, Multi agents (MARL)</li>
                <li> <b>Artificial life: cellular automata</b>:, open-ended evolution, major transisition, evolutionary algorithms</li>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr><h3> Thesis</h3></tr>

            <b>Towards open-ended dynamics in Artificial Life and Artificial Intelligence : an eco-evo-devo perspective.</b> <a href="data/thesis_hamon_gautier.pdf"> Thesis pdf </a>
            Defended on the 17th of march 2025.
            <tr><h3> Publications</h3></tr>
          

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div  class="one">
          <div  class="two" id='flowlenia_image'><video  width=100% muted autoplay loop>
          <source src="images/flowlenia.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
        </div>
        
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/flowlenia/">
          <span class="papertitle">Flow-Lenia: Towards open-ended evolution in cellular automata through mass conservation and parameter localization</span>
        </a>
        <br>
		<a href="">Erwan Plantec</a>,
		<strong>Gautier Hamon</strong>,
		<a href="https://mayalenetcheverry.com/">Mayalen Etcheverry</a>,
		<a href="http://www.pyoudeyer.com/">Pierre-Yves Oudeyer</a>,
		<a href="http://clement-moulin-frier.github.io/">Clément Moulin-Frier</a>,
        <a href="https://chakazul.github.io/">Bert Wang-Chak Chan</a>,
        <br>
        <em>Alife</em>, 2023 (<strong> Best paper award !</strong>)
        <br>
        <a href="https://sites.google.com/view/flowlenia/">project page</a>
        /
        <a href="https://direct.mit.edu/isal/proceedings/isal2023/35/131/116921">Paper</a>
        <p></p>
        <p>
        Introducing mass conservation and multi species simulation in a continuous cellular automaton leading to intrinsic evolution in the system.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div  class="one">
          <div  class="two" id='sensolenia_image'><video  width=100% muted autoplay loop>
          <source src="images/sensolenia.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
        </div>
        
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2402.10236">
          <span class="papertitle">Discovering Sensorimotor Agency in Cellular Automata using Diversity Search</span>
        </a>
        <br>
	  <strong>Gautier Hamon</strong>,
		<a href="https://mayalenetcheverry.com/">Mayalen Etcheverry</a>,
    <a href="http://clement-moulin-frier.github.io/">Clément Moulin-Frier</a>,
    <a href="https://chakazul.github.io/">Bert Wang-Chak Chan</a>,
		<a href="http://www.pyoudeyer.com/">Pierre-Yves Oudeyer</a>
	
        <br>
        <em>Arxiv</em>, 2023 (To appear in Science Advances)
        <br>
        <a href="https://developmentalsystems.org/sensorimotor-lenia/">Blogpost</a>
        /
        <a href="https://arxiv.org/abs/2402.10236">Paper</a>
        <p></p>
        <p>
          In this paper, we leverage recent advances in machine learning, combining algorithms for diversity search, curriculum learning and gradient descent, to automate the search in cellular automaton of parameter leading to the self-organization of
           localized structures that move around with the ability to react in a coherent manner to external obstacles and maintain their integrity, hence primitive forms of sensorimotor agency. The emerging macro agents does not possess a central control "brain" 
           but rather all the simple constituents self-organize into coherent entities capable of sensorimotor behavior. They also display impressive generalization capabilities as well as emergent behaviors.
        </p>
      </td>
    </tr>



    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div  class="one">
          <div  class="two" id='sensolenia_image'><video  width=100% muted autoplay loop>
          <source src="images/mutliA_metaRL.mp4" type="video/mp4">
          <!--<img src='images/multiagent_metaRL.jpg' width=100%>-->
          </video></div>
        </div>
        
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2311.00651">
          <span class="papertitle">Emergence of Collective Open-Ended Exploration from Decentralized Meta-Reinforcement Learning</span>
        </a>
        <br>
    <a href="">Richard Bornemann<sup>*</sup></a>,
		<strong>Gautier Hamon<sup>*</sup></strong>,
    <a href="https://eleninisioti.github.io">Eleni Nisioti</a>,
    <a href="http://clement-moulin-frier.github.io/">Clément Moulin-Frier</a>,
	
        <br>
        <em>Presented at <a href="https://sites.google.com/view/aloe2023">ALOE workshop</a> Neurips</em>, 2023 
        <br>
        <a href="https://sites.google.com/view/collective-open-ended-explore">Project page</a>
        /
        <a href="https://arxiv.org/abs/2311.00651">Paper</a>
        <p></p>
        <p>
          We introduce a novel environment with an open ended procedurally generated task space which dynamically combines multiple subtasks sampled from five diverse task types to form a vast distribution of task trees. We show that decentralized agents trained in our environment exhibit strong generalization abilities when confronted with novel objects at test time. Additionally, despite never being forced to cooperate during training the agents learn collective exploration strategies which allow them to solve novel tasks never encountered during training. We further find that the agents learned collective exploration strategies extend to an open ended task setting, allowing them to solve task trees of twice the depth compared to the ones seen during training
       </p>
      </td>
    </tr>




    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div  class="one">
          <img src='images/ecoevo_neuro.jpg' width=100%>
        </div>
        
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/abs/10.1145/3583133.3590703">
          <span class="papertitle">Eco-evolutionary Dynamics of Non-episodic Neuroevolution in Large Multi-agent Environments</span>
        </a>
        <br>
		<strong>Gautier Hamon</strong>,
    <a href="https://eleninisioti.github.io">Eleni Nisioti</a>,
    <a href="http://clement-moulin-frier.github.io/">Clément Moulin-Frier</a>,
	
        <br>
        <em>Gecco</em>, 2023 
        <br>
        <a href="https://sites.google.com/view/non-episodic-neuroevolution-in/">Project page</a>
        /
        <a href="https://dl.acm.org/doi/abs/10.1145/3583133.3590703">Paper</a>
        <p></p>
        <p>
          In this work we present a method for continuously evolving adaptive agents without any environment or population reset. The environment is a large grid world with complex spatiotemporal resource generation, containing many agents that are each controlled by an evolvable recurrent neural network and locally reproduce based on their internal physiology.
          The entire system is implemented in JAX, allowing very fast simulation on a GPU. We show that NE can operate in an ecologically-valid non-episodic multi-agent setting, finding sustainable collective foraging strategies in the presence of a complex interplay between ecological and evolutionary dynamics.
       </p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div  class="one">
          <img src='images/reservoir_RL.png' width=100%>
        </div>
        
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://link.springer.com/chapter/10.1007/978-3-031-56855-8_3">
          <span class="papertitle">Evolving Reservoirs for Meta Reinforcement Learning</span>
        </a>
        <br>
    <a href="">Corentin Léger<sup>*</sup></a>,
		<strong>Gautier Hamon<sup>*</sup></strong>,
    <a href="https://eleninisioti.github.io">Eleni Nisioti</a>,
    <a href="https://sites.google.com/site/xavierhinaut/"> Xavier Hinaut</a>
    <a href="http://clement-moulin-frier.github.io/">Clément Moulin-Frier</a>,
	
        <br>
        <em>EvoApps (part of EvoStar)</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2312.06695">Arxiv</a>
        <p></p>
        <p>
          We adopt a computational framework based on meta reinforcement learning as a model of the interplay between evolution and development. At the evolutionary scale, we evolve reservoirs, a family of recurrent neural networks that differ from conventional networks in that one optimizes not the synaptic weights, but hyperparameters controlling macro-level properties of the resulting network architecture. At the developmental scale, we employ these evolved reservoirs to facilitate the learning of a behavioral policy through Reinforcement Learning (RL). Within an RL agent, a reservoir encodes the environment state before providing it to an action policy. We evaluate our approach on several 2D and 3D simulated environments. Our results show that the evolution of reservoirs can improve the learning of diverse challenging tasks. 
       </p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div  class="one">
          
          <img src="images/multiAgent_IMGEP.gif" width=100%>

        </div>
        
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://proceedings.mlr.press/v232/nisioti23a.html">
          <span class="papertitle">Autotelic Reinforcement Learning in Multi-Agent Environments</span>
        </a>
        <br>
    <a href="">Elias Masquil<sup>*</sup></a>,
    <a href="https://eleninisioti.github.io">Eleni Nisioti<sup>*</sup></a>,
		<strong>Gautier Hamon<sup>*</sup></strong>,
    <a href="http://clement-moulin-frier.github.io/">Clément Moulin-Frier</a>,
	
        <br>
        <em>CoLLAs (Conference on Lifelong Learning Agents)</em>, 2023 
        <br>
        <a href="https://proceedings.mlr.press/v232/nisioti23a.html">Paper</a>
        <p></p>
        <p>
          In the intrinsically motivated skills acquisition problem, the agent is set in an environment with- out any pre-defined goals and needs to acquire an open-ended repertoire of skills. To do so the agent needs to be autotelic (deriving from the Greek auto (self) and telos (end goal)): it needs to generate goals and learn to achieve them following its own intrinsic motivation rather than external supervision.
          Multi-agent environments pose an additional challenge for autotelic agents: to discover and master goals that require cooperation agents must pursue them simultaneously, but they have low chances of doing so if they sample them independently.
           In this work, we propose a new learning paradigm for modelling such settings, the Decentralized Intrinsically Motivated Skills Acquisition Problem (Dec-IMSAP), and employ it to solve cooperative navigation tasks. First, we show that agents setting their goals independently fail to master the full diversity of goals. 
           Then, we show that a sufficient condition for achieving this is to ensure that a group aligns its goals, i.e., the agents pursue the same cooperative goal. Finally, we intro- duce the Goal-coordination game, a fully-decentralized emergent communication algorithm, where goal alignment emerges from the maximization of individual rewards in multi-goal cooperative environments and show that it is able to reach equal performance to a centralized training baseline that guarantees aligned goals.
       </p>
      </td>
    </tr>

    


    <!--
    <tr onmouseout="flowlenia_stop()" onmouseover="flowlenia_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='flowlenia_image'><video  width=100% muted autoplay loop>
          <source src="images/flowlenia.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/flowlenia.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function flowlenia_start() {
            document.getElementById('flowlenia_image').style.opacity = "1";
          }

          function flowlenia_stop() {
            document.getElementById('flowlenia_image').style.opacity = "0";
          }
          flowlenia_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/flowlenia/">
          <span class="papertitle">Flow-Lenia: Towards open-ended evolution in cellular automata through mass conservation and parameter localization</span>
        </a>
        <br>
		<a href="http://www.stronglyconvex.com/about.html">Erwan Plantec</a>,
		<a><strong>Gautier Hamon</strong></a>,
		<a href="https://creiser.github.io/">Christian Reiser</a>,
		<a href="">Peter Zhizhin</a>,
		<a href="">Jean-François Thibert</a>,
        <a href="https://lucic.ai/">Mario Lučić</a>,
        <a href="https://szeliski.org/">Richard Szeliski</a>,
        <br>
        <em>Alife</em>, 2023 (<strong> Best paper award !</strong>)
        <br>
        <a href="https://sites.google.com/view/flowlenia/">project page</a>
        /
        <a href="https://direct.mit.edu/isal/proceedings/isal2023/35/131/116921">Paper</a>
        <p></p>
        <p>
        Introducing mass conservation and multi species simulation in a continuous cellular automaton leading to intrinsic evolution in the system.
        </p>
      </td>
    </tr>--->
	

  


          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr><h3> Additional Open-Source code</h3></tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div  class="one">
                  <div  class="two" id='enter_sewer_slow'><video  width=100% muted autoplay loop>
                  <source src="images/enter_sewer_slow.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                </div>
                
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://github.com/Reytuag/transformerXL_PPO_JAX">
                  <span class="papertitle">TransformerXL for RL (PPO) in JAX</span>
                </a>
                <br>

            <strong>Gautier Hamon<sup>*</sup></strong>,
          
                <br>
                <em>Github</em>
                <br>
                <a href="https://github.com/Reytuag/transformerXL_PPO_JAX">Source code</a>
                <p></p>
                <p>
                  Jax implementation of the paper <a href="https://arxiv.org/abs/1910.06764">"Stabilizing Transformers for Reinforcement Learning"</a>. The code follows the <a href="https://github.com/luchris429/purejaxrl">PureJaxRL</a> template to be as clear as possible and can take any <a href=https://github.com/RobertTLange/gymnax>gymnax</a> environment. 
                  The code has been tested on <a href="https://github.com/MichaelTMatthews/Craftax">Craftax</a> on which it has set a new Sota being the first to obtain advanced acheivements (without much finetuning). Training a 5M transformer on Craftax for 1e9 steps takes 6h on a A100. On <a href="https://github.com/corl-team/xland-minigrid">Xland-minigrid</a> (where environment parallelization is even more advantageous), 
                  training on 16 000 parallel environments with a 5M transformer reaches speed of 250 000 steps per seconds on a single A100 (so 1e9 steps in a little bit more than an hour).
               </p>
              </td>
            </tr>

          
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template from <a href="https://github.com/jonbarron/jonbarron_website">here</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
